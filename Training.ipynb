{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example pipeline (one OSCAR file, no streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to change when working in different environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "from transformers import AutoTokenizer, RobertaTokenizerFast\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "import torch\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "# data files\n",
    "path = '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/Short/OSCAR_short.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available? False\n"
     ]
    }
   ],
   "source": [
    "# Check that PyTorch sees it\n",
    "print(f'Cuda available? {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_text_generator(gen):\n",
    "#     for i in gen:\n",
    "#         yield i['text']\n",
    "\n",
    "\n",
    "\n",
    "# oscar_short = load_dataset('text', data_files={\"train\": path}, split='train')\n",
    "# oscar_short_it = load_dataset('text', data_files={\"train\": path}, split='train', streaming=True)\n",
    "\n",
    "# p = '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/nl_part_1.txt'\n",
    "\n",
    "# oscar1_it = load_dataset('text', data_files={\"train\": p}, split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for data\n",
    "def load_datasets_from_paths(paths):\n",
    "    datasets_list = []\n",
    "\n",
    "    # Load each dataset from the provided paths\n",
    "    for path in paths:\n",
    "        dataset = load_dataset('text', data_files={\"train\": path}, split='train', streaming=True)\n",
    "        datasets_list.append(dataset)\n",
    "    \n",
    "    return datasets_list\n",
    "\n",
    "def combine_and_batch_datasets(datasets, batch_size=1000):\n",
    "    \"\"\"Yield batches of items from a list of iterable datasets.\"\"\"\n",
    "    for dataset in datasets:\n",
    "        batch = []\n",
    "        for item in dataset:\n",
    "            batch.append(item['text'])\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        # Yield any remaining items in the current dataset that didn't complete a full batch\n",
    "        if batch:\n",
    "            yield batch\n",
    "\n",
    "#paths = [str(x) for x in Path('/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR').glob(\"**/*.txt\")]\n",
    "\n",
    "\n",
    "#combined_generator = combine_and_batch_datasets(load_datasets_from_paths(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_1_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_2_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_3_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_4_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_5_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_6_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_7_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_8_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_9_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_10_converted.txt']\n",
    "\n",
    "\n",
    "\n",
    "p = ['/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_1_converted.txt']\n",
    "\n",
    "\n",
    "combined_generator1 = combine_and_batch_datasets(load_datasets_from_paths(p))\n",
    "combined_generator = combine_and_batch_datasets(load_datasets_from_paths(paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: train from old tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # set vocabulary size\n",
    "# vocab_size = 30000\n",
    "\n",
    "# # select dataset to train with\n",
    "# train_set = combined_generator\n",
    "# # train_set = create_text_generator(oscar_short_it)\n",
    "\n",
    "# # load an existing BPE tokenizer\n",
    "# old_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # retrain the tokenizer\n",
    "# tokenizer = old_tokenizer.train_new_from_iterator(train_set, vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/jan/Documents/Master/Thesis/Code/Tokenizers/extra30wp-10/tokenizer_config.json',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/Tokenizers/extra30wp-10/special_tokens_map.json',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/Tokenizers/extra30wp-10/vocab.txt',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/Tokenizers/extra30wp-10/added_tokens.json',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/Tokenizers/extra30wp-10/tokenizer.json')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('/Users/jan/Documents/Master/Thesis/Code/Tokenizers/extra30wp-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/Users/jan/Documents/Master/Thesis/Code/Tokenizers/extra30-10/tokenizer_config.json',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/Tokenizers/extra30-10/special_tokens_map.json',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/Tokenizers/extra30-10/vocab.json',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/Tokenizers/extra30-10/merges.txt',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/Tokenizers/extra30-10/added_tokens.json',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/Tokenizers/extra30-10/tokenizer.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = ['/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_1_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_2_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_3_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_4_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_5_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_6_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_7_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_8_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_9_converted.txt',\n",
    "         '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_10_converted.txt']\n",
    "\n",
    "\n",
    "\n",
    "p = ['/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/help_tokenizer/nl_part_1_converted.txt']\n",
    "\n",
    "\n",
    "combined_generator1 = combine_and_batch_datasets(load_datasets_from_paths(p))\n",
    "combined_generator = combine_and_batch_datasets(load_datasets_from_paths(paths))\n",
    "\n",
    "# set vocabulary size\n",
    "vocab_size = 30000\n",
    "\n",
    "# select dataset to train with\n",
    "train_set = combined_generator\n",
    "# train_set = create_text_generator(oscar_short_it)\n",
    "\n",
    "# load an existing BPE tokenizer\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# retrain the tokenizer\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(train_set, vocab_size)\n",
    "\n",
    "\n",
    "tokenizer.save_pretrained('/Users/jan/Documents/Master/Thesis/Code/Tokenizers/extra30-10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/jan/Documents/Master/Thesis/Code/tokenizer_config.json',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/special_tokens_map.json',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/vocab.json',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/merges.txt',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/added_tokens.json',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('/Users/jan/Documents/Master/Thesis/Code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: train from zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# voor één file:\n",
    "paths = path\n",
    "\n",
    "# voor meerdere files:\n",
    "#paths = [str(x) for x in Path(\"/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR\").glob(\"**/*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=20000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jan/Documents/Master/Thesis/Code/a/vocab.json',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/a/merges.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model('/Users/jan/Documents/Master/Thesis/Code/a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tokenizer\n",
    "\n",
    "(Note: this is not necessary, as we will load it in transformers to actually tokenize data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    '/Users/jan/Documents/Master/Thesis/trained_tokenizer/vocab.json',\n",
    "    '/Users/jan/Documents/Master/Thesis/trained_tokenizer/merges.txt',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBPE(RobertaTokenizerFast):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, text, *args, **kwargs):\n",
    "\n",
    "        # Call the parent class's __call__ method\n",
    "        encoded = super().__call__(text, *args, **kwargs)\n",
    "        \n",
    "        \n",
    "        # Add BOS and EOS tokens to the input_ids\n",
    "        if 'input_ids' in encoded:\n",
    "            encoded['input_ids'] = [self.bos_token_id] + encoded['input_ids'] + [self.eos_token_id]\n",
    "        \n",
    "        # If attention_mask is present, adjust it as well\n",
    "        if 'attention_mask' in encoded:\n",
    "            encoded['attention_mask'] = [1] + encoded['attention_mask'] + [1]\n",
    "        \n",
    "        return encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'CustomBPE'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # WITH special tokens in call output\n",
    "tokenizer_bpe = CustomBPE.from_pretrained(\"/Users/jan/Documents/Master/Thesis/Code/Tokenizers/tokenizeBPE/t50\", max_length=512, truncation=True)\n",
    "\n",
    "special_tokens = {\n",
    "    'pad_token': '<pad>',\n",
    "    'bos_token': '<s>',\n",
    "    'eos_token': '</s>',\n",
    "    'unk_token': '<unk>',\n",
    "    'mask_token': '<mask>',\n",
    "}\n",
    "\n",
    "tokenizer_bpe.add_special_tokens(special_tokens)\n",
    "\n",
    "# WITHOUT special tokens in call output\n",
    "# tokenizer_bpe = RobertaTokenizerFast.from_pretrained(\"/Users/jan/Documents/Master/Thesis/Code/Tokenizers/tokenizeBPE/t50\", max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bpe.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v = tokenizer_bpe.get_vocab()\n",
    "g = {value: key for key, value in v.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "\n",
    "class CustomWP(RobertaTokenizerFast):\n",
    "    def __init__(self, **kwargs):\n",
    "        \n",
    "        # Call the parent class's __init__ method with all keyword arguments\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def __call__(self, text, return_tensors=None, **kwargs):\n",
    "        # Call the parent class's __call__ method to get the encoded result\n",
    "        encoding = super().__call__(text, **kwargs)\n",
    "        \n",
    "        def trim_tokens(tokens):\n",
    "            # Only trim if there are more than two tokens (to avoid empty sequences)\n",
    "            return tokens[1:-1] if len(tokens) > 2 else tokens\n",
    "\n",
    "        # Check and process input_ids\n",
    "        if 'input_ids' in encoding:\n",
    "            if isinstance(encoding['input_ids'][0], list):\n",
    "                encoding['input_ids'] = [self.bos_token_id] + [trim_tokens(seq) for seq in encoding['input_ids']] + [self.eos_token_id]\n",
    "            else:\n",
    "                encoding['input_ids'] = [self.bos_token_id] + trim_tokens(encoding['input_ids']) + [self.eos_token_id]\n",
    "        \n",
    "        # # Check and process attention_mask\n",
    "        # if 'attention_mask' in encoding:\n",
    "        #     if isinstance(encoding['attention_mask'][0], list):\n",
    "        #         encoding['attention_mask'] = [trim_tokens(mask) for mask in encoding['attention_mask']]\n",
    "        #     else:\n",
    "        #         encoding['attention_mask'] = trim_tokens(encoding['attention_mask'])\n",
    "\n",
    "        if return_tensors == 'pt':\n",
    "            encoding['input_ids'] = torch.tensor(encoding['input_ids'])\n",
    "        return encoding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'CustomWP'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# WITHOUT special tokens in call output\n",
    "tokenizer_wp = CustomWP.from_pretrained(\"/Users/jan/Documents/Master/Thesis/Code/Tokenizers/tokenizeWP/t50\", max_len=512)\n",
    "\n",
    "# WITH special tokens in call output\n",
    "#tokenizer_wp = RobertaTokenizerFast.from_pretrained(\"/Users/jan/Documents/Master/Thesis/Code/Tokenizers/tokenizeWP/t50\", max_len=512)\n",
    "\n",
    "special_tokens = {\n",
    "    'pad_token': '<pad>',\n",
    "    'bos_token': '<s>',\n",
    "    'eos_token': '</s>',\n",
    "    'unk_token': '<unk>',\n",
    "    'mask_token': '<mask>',\n",
    "}\n",
    "\n",
    "tokenizer_wp.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 3: Custom Tokenizer BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must first define our own tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "class CustomTokenizerBPE:\n",
    "\n",
    "    def __init__(self, segmentation_dictionary, bpe_tokenizer, max_length=512, pad_to_multiple_of=None):\n",
    "        \n",
    "        self.bpe_tokenizer = bpe_tokenizer\n",
    "        self.bpe_vocab = self.bpe_tokenizer.get_vocab()\n",
    "\n",
    "        self.segmentations = {word: seg for word, seg in segmentation_dictionary.items() if len(seg) > 0}\n",
    "        self.seg_dict = {}\n",
    "        for word, segs in self.segmentations.items():\n",
    "            out = []\n",
    "            for i, seg in enumerate(segs):\n",
    "                if i == 0:\n",
    "                    out.append('Ġ' + seg)\n",
    "                else:\n",
    "                    out.append(seg)\n",
    "            self.seg_dict[word] = out\n",
    "        \n",
    "        self.segments = {seg for segs in self.seg_dict.values() for seg in segs}\n",
    "        # self.seg_vocab = {seg: (i + len(self.bpe_vocab)) for i, seg in enumerate(self.segments) if not seg in self.bpe_vocab}\n",
    "        \n",
    "        # self.vocab = self.bpe_vocab | self.seg_vocab\n",
    "\n",
    "        self.vocab = self.bpe_vocab.copy()\n",
    "        \n",
    "        for element in self.segments:\n",
    "            if element not in self.vocab:\n",
    "                self.vocab[element] = len(self.vocab) + 1\n",
    "\n",
    "        self.mask_token = \"<mask>\"\n",
    "        self.mask_token_id = self.vocab['<mask>']\n",
    "        self.vocab[self.mask_token_id] = self.vocab['<mask>']\n",
    "        \n",
    "        self.cls_token = \"<s>\"\n",
    "        self.cls_token_id = self.vocab['<s>']\n",
    "        self.vocab[self.cls_token] = self.vocab['<s>']\n",
    "        \n",
    "        self.sep_token = \"</s>\"\n",
    "        self.sep_token_id = self.vocab['</s>']\n",
    "        self.vocab[self.sep_token] = self.vocab['</s>']\n",
    "        \n",
    "        self.pad_token = '<pad>'\n",
    "        self.pad_token_id = self.vocab['<pad>']\n",
    "        self.vocab[self.pad_token] = self.vocab['<pad>']\n",
    "        \n",
    "\n",
    "\n",
    "        # self.unk_token = '<|endoftext|>'\n",
    "        # self.unk_token_id = self.vocab['<|endoftext|>']\n",
    "        # self.vocab[self.unk_token] = self.vocab['<|endoftext|>']\n",
    "        \n",
    "        self.vocab['<unk>'] = len(self.vocab) + 1 \n",
    "        self.unk_token = '<unk>'\n",
    "        self.unk_token_id = self.vocab['<unk>']\n",
    "        self.vocab[self.unk_token] = self.vocab['<unk>']\n",
    "        \n",
    "        # self.bos_token = '<|endoftext|>'\n",
    "        # self.bos_token_id = self.vocab['<|endoftext|>']\n",
    "        # self.vocab[self.bos_token] = self.vocab['<|endoftext|>']\n",
    "\n",
    "        self.bos_token = '<s>'\n",
    "        self.bos_token_id = self.vocab['<s>']\n",
    "        self.vocab[self.bos_token] = self.vocab['<s>']\n",
    "        \n",
    "        self.eos_token = \"</s>\"\n",
    "        self.eos_token_id = self.vocab[\"</s>\"]\n",
    "        self.vocab[self.eos_token] = self.vocab[\"</s>\"]\n",
    "        \n",
    "        self.special_tokens = [self.vocab['<mask>'], self.vocab['<s>'], self.vocab['</s>'], self.vocab['<pad>'], self.vocab['<unk>']]\n",
    "   \n",
    "        \n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        self.inverted_vocab = {value: key for key, value in self.vocab.items()}\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "\n",
    "    def tokenize(self, seq):\n",
    "\n",
    "        # Implement your token to ID conversion logic here\n",
    "        text = self.bpe_tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(seq)\n",
    "        words_a = [word for word in [word for word, offset in text]]\n",
    "        \n",
    "        words_b = [word.replace('Ġ', '') for word in [word for word, offset in text]]\n",
    "        tokens = []\n",
    "        for i, word_b in enumerate(words_b):\n",
    "            if word_b in self.seg_dict:\n",
    "                tokens += self.seg_dict[word_b]\n",
    "            else:\n",
    "                if words_a[i][0] == 'Ġ':\n",
    "                    tokens += self.bpe_tokenizer.tokenize(' ' + word_b)\n",
    "                else: \n",
    "                    tokens += self.bpe_tokenizer.tokenize(word_b)    \n",
    "        return tokens\n",
    "        \n",
    "        \n",
    "    def encode(self, seq):\n",
    "\n",
    "\n",
    "        return [self.bos_token_id] + self.convert_tokens_to_ids(self.tokenize(seq)) + [self.eos_token_id]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        if token in self.vocab:\n",
    "            return self.vocab[token]\n",
    "        else:\n",
    "            return self.unk_token_id\n",
    "\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        if isinstance(tokens, list):\n",
    "            return [self._convert_token_to_id(token) for token in tokens]\n",
    "        return self._convert_token_to_id(tokens)\n",
    "\n",
    "\n",
    "    def _tokenize(self, seq):\n",
    "        text = self.bpe_tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(seq)\n",
    "        words_a = [word for word in [word for word, offset in text]]\n",
    "        \n",
    "        words_b = [word.replace('Ġ', '') for word in [word for word, offset in text]]\n",
    "        tokens = []\n",
    "        for i, word_b in enumerate(words_b):\n",
    "            if word_b in self.seg_dict:\n",
    "                tokens += self.seg_dict[word_b]\n",
    "            else:\n",
    "                if words_a[i][0] == 'Ġ':\n",
    "                    tokens += self.bpe_tokenizer.tokenize(' ' + word_b)\n",
    "                else: \n",
    "                    tokens += self.bpe_tokenizer.tokenize(word_b)    \n",
    "        return tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, text, return_tensors=None, padding=False, truncation=False, max_length=None, add_special_tokens=False):\n",
    "        if isinstance(text, list):\n",
    "            self.batch_encode_plus(text, truncation=truncation, return_tensors=return_tensors, padding=padding, max_length=max_length, add_special_tokens=add_special_tokens)\n",
    "        else:\n",
    "\n",
    "            token_ids = self.encode(text) \n",
    "\n",
    "\n",
    "            if truncation and max_length:\n",
    "\n",
    "                if len(token_ids) > max_length:\n",
    "                    token_ids = token_ids[:max_length - 1] + [self.eos_token_id]\n",
    "\n",
    "            # if truncation and max_length:\n",
    "            #     # token_ids = [self.bos_token_id] + token_ids[:max_length - 2] + [self.eos_token_id]\n",
    "            #     # token_ids = token_ids[:max_length]\n",
    "            #     token_ids = [self.bos_token_id] + token_ids[:max_length - 2] + [self.eos_token_id]\n",
    "\n",
    "            if return_tensors == \"pt\":\n",
    "                return {\"input_ids\": torch.tensor(token_ids, dtype=torch.long)}\n",
    "            return {\"input_ids\": token_ids}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _convert_id_to_token(self, id):\n",
    "        # Implement your ID to token conversion logic here\n",
    "        return self.inverted_vocab[id]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        # Convert IDs back to tokens\n",
    "        if isinstance(ids, list):\n",
    "            return [self._convert_id_to_token(id) for id in ids]\n",
    "        return self._convert_id_to_token(ids)\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        out = ''\n",
    "        for id in ids:\n",
    "            word = self._convert_id_to_token(id)\n",
    "            if word[0] == 'Ġ':\n",
    "                out += word.replace('Ġ', ' ')\n",
    "            else:\n",
    "                out += word\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def get_special_tokens_mask(self, token_ids, already_has_special_tokens=False):\n",
    "        # Create a mask for special tokens\n",
    "        return [1 if self._is_special_token(token_id) else 0 for token_id in token_ids]\n",
    "\n",
    "\n",
    "    def _is_special_token(self, token_id):\n",
    "        # Implement your logic to check if a token is a special token\n",
    "        if token_id in self.special_tokens:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def batch_encode_plus(self, texts, return_tensors=None, padding=False, truncation=False, max_length=None, add_special_tokens=False):\n",
    "        batch_token_ids = [self.__call__(text, truncation=truncation, max_length=max_length, add_special_tokens=add_special_tokens)[\"input_ids\"] for text in texts]\n",
    "        \n",
    "        if padding:\n",
    "            if max_length is None:\n",
    "                max_length = max(len(ids) for ids in batch_token_ids)\n",
    "            batch_token_ids = [ids + [self.pad_token_id] * (max_length - len(ids)) for ids in batch_token_ids]\n",
    "        \n",
    "        if return_tensors == \"pt\":\n",
    "            return {\"input_ids\": torch.tensor(batch_token_ids, dtype=torch.long)}\n",
    "        return {\"input_ids\": batch_token_ids}\n",
    "\n",
    "\n",
    "    def pad(self, batch, return_tensors=\"pt\", pad_to_multiple_of=None):\n",
    "        if pad_to_multiple_of is None:\n",
    "            pad_to_multiple_of = self.pad_to_multiple_of\n",
    "\n",
    "\n",
    "        input_ids_list = []\n",
    "        for dictionary in batch:\n",
    "            for key, value in dictionary.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    input_ids_list.append(value.tolist())\n",
    "\n",
    "\n",
    "        max_length = max(len(x) for x in input_ids_list)\n",
    "        \n",
    "        if pad_to_multiple_of is not None:\n",
    "            max_length = (max_length + pad_to_multiple_of - 1) // pad_to_multiple_of * pad_to_multiple_of\n",
    "        \n",
    "        padded_batch = []\n",
    "        for seq in input_ids_list:\n",
    "            if len(seq) < max_length:\n",
    "                seq.extend([self.pad_token_id] * (max_length - len(seq)))\n",
    "            padded_batch.append(seq)\n",
    "        \n",
    "        attention_list = []\n",
    "        for inner_list in padded_batch:\n",
    "            p_list = [1 if value < self.vocab_size else 0 for value in inner_list]\n",
    "            attention_list.append(p_list)\n",
    "        \n",
    "        if return_tensors == \"pt\":\n",
    "            return {'input_ids': torch.tensor(padded_batch, dtype=torch.long), 'attention_mask': torch.tensor(attention_list, dtype=torch.long)}\n",
    "        \n",
    "        return {'input_ids': padded_batch, 'attention_mask': attention_list}\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the segmentation dictionary and help tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "path_to_dict = '/Users/jan/Documents/Master/Thesis/Code/seg_dict.json'\n",
    "path_to_tokenizer = \"/Users/jan/Documents/Master/Thesis/Code/Tokenizers/extra30-10\"\n",
    "\n",
    "import json\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        my_dict = json.load(f)\n",
    "    return my_dict\n",
    "\n",
    "\n",
    "segmentation_dictionary= load_json(path_to_dict)\n",
    "help_tokenizer = RobertaTokenizerFast.from_pretrained(path_to_tokenizer)\n",
    "\n",
    "\n",
    "tokenizer_own_bpe = CustomTokenizerBPE(segmentation_dictionary, help_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 4: Custom tokenizer WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizerWP:\n",
    "\n",
    "    def __init__(self, segmentation_dictionary, wp_tokenizer, max_length=512, pad_to_multiple_of=None):\n",
    "        \n",
    "        self.wp_tokenizer = wp_tokenizer\n",
    "        self.wp_vocab = self.wp_tokenizer.get_vocab()\n",
    "\n",
    "        self.segmentations = {word: seg for word, seg in segmentation_dictionary.items() if len(seg) > 0}\n",
    "        self.seg_dict = {}\n",
    "        for word, segs in self.segmentations.items():\n",
    "            out = []\n",
    "            for i, seg in enumerate(segs):\n",
    "                if i == 0:\n",
    "                    out.append(seg)\n",
    "                else:\n",
    "                    out.append('##' + seg)\n",
    "            self.seg_dict[word] = out\n",
    "        \n",
    "        self.segments = {seg for segs in self.seg_dict.values() for seg in segs}\n",
    "        # self.seg_vocab = {seg: (i + len(self.bpe_vocab)) for i, seg in enumerate(self.segments) if not seg in self.bpe_vocab}\n",
    "        \n",
    "        # self.vocab = self.bpe_vocab | self.seg_vocab\n",
    "\n",
    "        self.vocab = self.wp_vocab.copy()\n",
    "        \n",
    "        for element in self.segments:\n",
    "            if element not in self.vocab:\n",
    "                self.vocab[element] = len(self.vocab) + 1\n",
    "        \n",
    "\n",
    "        self.l = len(self.vocab)\n",
    "\n",
    "        self.vocab['<unk>'] = self.l + 1 \n",
    "        self.unk_token = '<unk>'\n",
    "        self.unk_token_id = self.vocab['<unk>']\n",
    "        self.vocab[self.unk_token] = self.vocab['<unk>']\n",
    "\n",
    "        self.vocab['<mask>'] = self.l + 2 \n",
    "        self.mask_token = \"<mask>\"\n",
    "        self.mask_token_id = self.vocab['<mask>']\n",
    "        self.vocab[self.mask_token_id] = self.vocab['<mask>']\n",
    "        \n",
    "        self.vocab['<s>'] = self.l + 3\n",
    "        self.cls_token = \"<s>\"\n",
    "        self.cls_token_id = self.vocab['<s>']\n",
    "        self.vocab[self.cls_token] = self.vocab['<s>']\n",
    "        \n",
    "        self.vocab['</s>'] = self.l + 4\n",
    "        self.sep_token = \"</s>\"\n",
    "        self.sep_token_id = self.vocab['</s>']\n",
    "        self.vocab[self.sep_token] = self.vocab['</s>']\n",
    "        \n",
    "        self.vocab['<pad>'] = self.l + 5\n",
    "        self.pad_token = '<pad>'\n",
    "        self.pad_token_id = self.vocab['<pad>']\n",
    "        self.vocab[self.pad_token] = self.vocab['<pad>']\n",
    "        \n",
    "        self.vocab['<s>'] = self.l + 6\n",
    "        self.bos_token = '<s>'\n",
    "        self.bos_token_id = self.vocab['<s>']\n",
    "        self.vocab[self.bos_token] = self.vocab['<s>']\n",
    "        \n",
    "        self.vocab['</s>'] = self.l + 7 \n",
    "        self.eos_token = \"</s>\"\n",
    "        self.eos_token_id = self.vocab[\"</s>\"]\n",
    "        self.vocab[self.eos_token] = self.vocab[\"</s>\"]\n",
    "        \n",
    "        self.special_tokens = [self.vocab['<mask>'], self.vocab['<s>'], self.vocab['</s>'], self.vocab['<pad>'], self.vocab['<unk>']]\n",
    "   \n",
    "        \n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        self.inverted_vocab = {value: key for key, value in self.vocab.items()}\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "\n",
    "    def tokenize(self, seq):\n",
    "\n",
    "\n",
    "        text = self.wp_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(seq)\n",
    "        words_a = [word for word in [word for word, offset in text]]\n",
    "        \n",
    "        words_b = [word.replace('##', '') for word in [word for word, offset in text]]\n",
    "        tokens = []\n",
    "        for i, word_b in enumerate(words_b):\n",
    "            if word_b in self.seg_dict:\n",
    "                tokens += self.seg_dict[word_b]\n",
    "            else:\n",
    "                if words_a[i][0] == '##':\n",
    "                    tokens += self.wp_tokenizer.tokenize(word_b)\n",
    "                else: \n",
    "                    tokens += self.wp_tokenizer.tokenize(' ' + word_b)    \n",
    "        return tokens\n",
    "        \n",
    "        \n",
    "    def encode(self, seq):\n",
    "\n",
    "\n",
    "        return [self.bos_token_id] + self.convert_tokens_to_ids(self.tokenize(seq)) + [self.eos_token_id]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        if token in self.vocab:\n",
    "            return self.vocab[token]\n",
    "        else:\n",
    "            return self.unk_token_id\n",
    "\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        if isinstance(tokens, list):\n",
    "            return [self._convert_token_to_id(token) for token in tokens]\n",
    "        return self._convert_token_to_id(tokens)\n",
    "\n",
    "\n",
    "    def _tokenize(self, seq):\n",
    "\n",
    "        text = self.wp_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(seq)\n",
    "        words_a = [word for word in [word for word, offset in text]]\n",
    "        \n",
    "        words_b = [word.replace('##', '') for word in [word for word, offset in text]]\n",
    "        tokens = []\n",
    "        for i, word_b in enumerate(words_b):\n",
    "            if word_b in self.seg_dict:\n",
    "                tokens += self.seg_dict[word_b]\n",
    "            else:\n",
    "                if words_a[i][0] == '##':\n",
    "                    tokens += self.wp_tokenizer.tokenize(word_b)\n",
    "                else: \n",
    "                    tokens += self.wp_tokenizer.tokenize(' ' + word_b)    \n",
    "        return tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, text, return_tensors=None, padding=False, truncation=False, max_length=None, add_special_tokens=False):\n",
    "        if isinstance(text, list):\n",
    "            self.batch_encode_plus(text, truncation=truncation, return_tensors=return_tensors, padding=padding, max_length=max_length, add_special_tokens=add_special_tokens)\n",
    "        else:\n",
    "\n",
    "            token_ids = self.encode(text) \n",
    "\n",
    "            if truncation and max_length:\n",
    "\n",
    "                if len(token_ids) > max_length:\n",
    "                    token_ids = token_ids[:max_length - 1] + [self.eos_token_id]\n",
    "\n",
    "\n",
    "            # if padding and max_length is not None:\n",
    "            #     token_ids = token_ids + [self.pad_token_id] * (max_length - len(token_ids))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if return_tensors == \"pt\":\n",
    "                return {\"input_ids\": torch.tensor(token_ids, dtype=torch.long)}\n",
    "            return {\"input_ids\": token_ids}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _convert_id_to_token(self, id):\n",
    "        # Implement your ID to token conversion logic here\n",
    "        return self.inverted_vocab[id]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        # Convert IDs back to tokens\n",
    "        if isinstance(ids, list):\n",
    "            return [self._convert_id_to_token(id) for id in ids]\n",
    "        return self._convert_id_to_token(ids)\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        out = ''\n",
    "        for id in ids:\n",
    "            word = self._convert_id_to_token(id)\n",
    "            if word[:2] == '##':\n",
    "                out += word.replace('##', '')\n",
    "            else:\n",
    "                out += ' ' + word\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def get_special_tokens_mask(self, token_ids, already_has_special_tokens=False):\n",
    "        # Create a mask for special tokens\n",
    "        return [1 if self._is_special_token(token_id) else 0 for token_id in token_ids]\n",
    "\n",
    "\n",
    "    def _is_special_token(self, token_id):\n",
    "        # Implement your logic to check if a token is a special token\n",
    "        if token_id in self.special_tokens:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def batch_encode_plus(self, texts, return_tensors=None, padding=False, truncation=False, max_length=None, add_special_tokens=False):\n",
    "        batch_token_ids = [self.__call__(text, truncation=truncation, max_length=max_length, add_special_tokens=add_special_tokens)[\"input_ids\"] for text in texts]\n",
    "        \n",
    "        if padding:\n",
    "            if max_length is None:\n",
    "                max_length = max(len(ids) for ids in batch_token_ids)\n",
    "            batch_token_ids = [ids + [self.pad_token_id] * (max_length - len(ids)) for ids in batch_token_ids]\n",
    "        \n",
    "        if return_tensors == \"pt\":\n",
    "            return {\"input_ids\": torch.tensor(batch_token_ids, dtype=torch.long)}\n",
    "        return {\"input_ids\": batch_token_ids}\n",
    "\n",
    "\n",
    "    def pad(self, batch, return_tensors=\"pt\", pad_to_multiple_of=None):\n",
    "        if pad_to_multiple_of is None:\n",
    "            pad_to_multiple_of = self.pad_to_multiple_of\n",
    "\n",
    "\n",
    "        input_ids_list = []\n",
    "        for dictionary in batch:\n",
    "            for key, value in dictionary.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    input_ids_list.append(value.tolist())\n",
    "\n",
    "\n",
    "        #max_length = self.max_length or max(len(x) for x in input_ids_list)\n",
    "        max_length = max(len(x) for x in input_ids_list)\n",
    "        \n",
    "        if pad_to_multiple_of is not None:\n",
    "            max_length = (max_length + pad_to_multiple_of - 1) // pad_to_multiple_of * pad_to_multiple_of\n",
    "        \n",
    "        padded_batch = []\n",
    "        for seq in input_ids_list:\n",
    "            if len(seq) < max_length:\n",
    "                seq.extend([self.pad_token_id] * (max_length - len(seq)))\n",
    "            padded_batch.append(seq)\n",
    "        \n",
    "        attention_list = []\n",
    "        for inner_list in padded_batch:\n",
    "            p_list = [1 if value < self.vocab_size else 0 for value in inner_list]\n",
    "            attention_list.append(p_list)\n",
    "        \n",
    "        if return_tensors == \"pt\":\n",
    "            return {'input_ids': torch.tensor(padded_batch, dtype=torch.long), 'attention_mask': torch.tensor(attention_list, dtype=torch.long)}\n",
    "        \n",
    "        return {'input_ids': padded_batch, 'attention_mask': attention_list}\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "path_to_dict = '/Users/jan/Documents/Master/Thesis/Code/seg_dict.json'\n",
    "path_to_tokenizer = \"/Users/jan/Documents/Master/Thesis/Code/Tokenizers/extra30wp-10\"\n",
    "\n",
    "import json\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        my_dict = json.load(f)\n",
    "    return my_dict\n",
    "\n",
    "\n",
    "segmentation_dictionary = load_json(path_to_dict)\n",
    "help_tokenizer = RobertaTokenizerFast.from_pretrained(path_to_tokenizer)\n",
    "\n",
    "tokenizer_own_wp = CustomTokenizerWP(segmentation_dictionary, help_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50016"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_own_wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30002"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(help_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: LineByLineTextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 397 ms, sys: 15.3 ms, total: 412 ms\n",
      "Wall time: 89.6 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset_a = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer_wp,\n",
    "    file_path='/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/Short/OSCAR_shorter.txt',\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [30002, 53, 40854, 698, 2528, 1325, 290, 5456, 84, 4886, 974, 532, 24079, 257, 14, 221, 52, 688, 642, 491, 314, 11741, 12, 626, 2105, 642, 4886, 3312, 47064, 749, 11445, 29745, 30000]}\n",
      "{'input_ids': [30002, 24, 569, 356, 40415, 83, 6080, 257, 16672, 14, 221, 41, 78, 3648, 1938, 12, 642, 626, 2105, 4886, 3874, 569, 835, 7512, 12, 3312, 569, 8468, 626, 2105, 642, 293, 742, 84, 440, 40415, 492, 314, 1418, 6913, 532, 25616, 14, 30000]}\n",
      "{'input_ids': [30002, 25, 221, 46, 533, 1301, 642, 14563, 374, 7421, 8832, 257, 26664, 356, 369, 495, 4993, 268, 12, 8184, 642, 9280, 257, 5701, 1418, 412, 674, 357, 824, 12, 769, 314, 4886, 500, 974, 11412, 257, 14, 30000]}\n",
      "OSCAR_shorter2_tokenized\n",
      "CPU times: user 1.75 s, sys: 16.2 ms, total: 1.76 s\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# pick tokenizer\n",
    "tokenizer = tokenizer_own_bpe\n",
    "\n",
    "# pick txt file to transform\n",
    "text_file_paths = ['/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/Short/OSCAR_shorter2.txt']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def tokenize function\n",
    "def tokenize_line(line):\n",
    "    # Specify the max_length parameter to ensure consistent padding\n",
    "    return {'input_ids': tokenizer(line.strip(), add_special_tokens=True, truncation=True, max_length=512)['input_ids']}\n",
    "    #return tokenizer(line, truncation=False, max_length=512, return_tensors='pt', add_special_tokens=False)\n",
    "\n",
    "# , max_length=512, truncation=True, return_tensors='pt'\n",
    "\n",
    "# Read the text file and tokenize each line\n",
    "\n",
    "for text_file_path in text_file_paths:\n",
    "    with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Create a list of dictionaries with tokenized lines\n",
    "    # tokenized_lines = []\n",
    "    # for line in lines:\n",
    "    #     t = tokenize_line(line)\n",
    "    #     if len\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    tokenized_lines = [tokenize_line(line) for line in lines]\n",
    "    \n",
    "    print(tokenized_lines[0])\n",
    "    print(tokenized_lines[1])\n",
    "    print(tokenized_lines[2])\n",
    "\n",
    "    dataset = Dataset.from_list(tokenized_lines).with_format(\"torch\")\n",
    "\n",
    "    base_name = text_file_path.strip('.txt')\n",
    "\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(text_file_path))[0]\n",
    "    \n",
    "\n",
    "    dataset_name = f'{base_name}_tokenized'\n",
    "    p = f'/Users/jan/Documents/Master/Thesis/Code/Datasets/Tokenized/WP/{dataset_name}'\n",
    "    print(dataset_name)\n",
    "\n",
    "    #save\n",
    "    #dataset.save_to_disk(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 512\n",
      "at nr 94\n"
     ]
    }
   ],
   "source": [
    "def test_length(dataset):\n",
    "    mx = 0\n",
    "    for i in range(999):\n",
    "        if dataset[i]['input_ids'].size()[0] > mx:\n",
    "            mx = dataset[i]['input_ids'].size()[0]\n",
    "            n = i\n",
    "    print('max length:', mx)\n",
    "    print('at nr', n)\n",
    "\n",
    "test_length(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [dataset[0], dataset[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[30002,    53, 40854,   698,  2528,  1325,   290,  5456,    84,  4886,\n",
       "            974,   532, 24079,   257,    14,   221,    52, 30003,   642,   491,\n",
       "            314, 11741,    12,   626,  2105,   642,  4886,  3312,  3283,   749,\n",
       "          32868, 30003, 30000, 30001, 30001, 30001, 30001, 30001, 30001, 30001,\n",
       "          30001, 30001, 30001, 30001],\n",
       "         [30002,    24,   569,   356, 40415,    83,  6080,   257, 16672,    14,\n",
       "            221,    41, 30003,  3648,  1938,    12, 30003,   626,  2105,  4886,\n",
       "           3874,   569,   835, 30003, 10981,  3312, 30003,  8468,   626,  2105,\n",
       "          30003,   293,   742,    84,   440, 40415,   492,   314,  1418,  6913,\n",
       "            532, 25616,    14, 30000]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  1325,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,   688,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 47064,  -100,\n",
       "          11445, 29745,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,    78,  -100,  -100,  -100,   642,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  7512,    12,  -100,   569,  -100,  -100,  -100,\n",
       "            642,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100]])}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{v: k for k, v in tokenizer.get_vocab().items()}[50001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest element is: 50284\n"
     ]
    }
   ],
   "source": [
    "def find_largest_element(tensors):\n",
    "    largest = float('-inf')  # Initialize to negative infinity\n",
    "    for tensor in tensors:\n",
    "        max_in_tensor = torch.max(tensor)\n",
    "        if max_in_tensor > largest:\n",
    "            largest = max_in_tensor.item()\n",
    "    return largest\n",
    "\n",
    "# List of tensors\n",
    "tensors = [dd[i]['input_ids'] for i in range(1000)]\n",
    "\n",
    "# Find and print the largest element\n",
    "largest_element = find_largest_element(tensors)\n",
    "print(\"The largest element is:\", largest_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validate_collator_output(batch, vocab_size):\n",
    "\n",
    "#     input_ids = batch['input_ids']\n",
    "#     labels = batch['labels']\n",
    "#     attention_mask = batch['attention_mask']\n",
    "    \n",
    "#     # check that all tensors have the same shape\n",
    "#     if input_ids.shape != labels.shape or input_ids.shape != attention_mask.shape:\n",
    "#         print(f\"Shape mismatch: input_ids shape {input_ids.shape}, labels shape {labels.shape}, attention_mask shape {attention_mask.shape}\")\n",
    "#         return False\n",
    "    \n",
    "#     # check each tensor element\n",
    "#     for i in range(input_ids.size(0)):  \n",
    "#         for j in range(input_ids.size(1)):\n",
    "#             input_id = input_ids[i, j].item()\n",
    "#             label = labels[i, j].item()\n",
    "#             attention = attention_mask[i, j].item()\n",
    "\n",
    "#             # # check that labels match input_ids or are -100\n",
    "#             # if label != -100 and input_id != tokenizer.mask_token_id:\n",
    "#             #     print(f\"Label mismatch at batch {i}, position {j}: label {label}, input_id {input_id}\")\n",
    "#             #     return False\n",
    "            \n",
    "#             # check that labels match input_ids or are -100\n",
    "#             if label != -100 and input_id != tokenizer.mask_token_id:\n",
    "#                 print(f\"Label mismatch at batch {i}, position {j}: label {label}, input_id {input_id}\")\n",
    "#                 return False\n",
    "            \n",
    "#             # check that all input_ids and labels are within the vocabulary range\n",
    "#             if input_id >= vocab_size or (label != -100 and label >= vocab_size):\n",
    "#                 print(f\"Out of range token at batch {i}, position {j}: input_id {input_id}, label {label}\")\n",
    "#                 return False\n",
    "            \n",
    "#             # check that attention_mask is 1 for non-padded tokens and 0 for padded tokens\n",
    "#             if (input_id != tokenizer.pad_token_id and attention != 1) or (input_id == tokenizer.pad_token_id and attention != 0):\n",
    "#                 print(f\"Attention mask mismatch at batch {i}, position {j}: input_id {input_id}, attention {attention}\")\n",
    "#                 return False\n",
    "    \n",
    "#     print(\"Validation passed.\")\n",
    "#     return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_own_wp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters model: 38756096\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=len(tokenizer.get_vocab()),\n",
    "    max_position_embeddings=512,\n",
    "    num_attention_heads=8,\n",
    "    num_hidden_layers=8,\n",
    "    type_vocab_size=1, \n",
    "    hidden_size=512,\n",
    "    intermediate_size=1024\n",
    ")\n",
    "\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "print(f'number of parameters model: {model.num_parameters()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Users/jan/Documents/Master/Thesis/Code/Models/model3\", # note: this is not where the model is saved, just info about training\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=1000,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "# training_args = {\n",
    "#     \"learning_rate\": 5e-5,\n",
    "#     \"warmup_steps\": 10000,  # 10% of 100,000 training steps\n",
    "#     \"total_training_steps\": 100000,\n",
    "#     \"batch_size\": 16,  # adjust based on GPU memory\n",
    "#     \"gradient_accumulation_steps\": 2,  # effectively increases batch size to 32\n",
    "#     \"num_train_epochs\": 3,  # adjust based on dataset and desired thoroughness\n",
    "#     \"optimizer\": \"AdamW\",\n",
    "#     \"weight_decay\": 0.01,\n",
    "#     \"gradient_clipping\": 1.0,\n",
    "#     \"loss_function\": \"CrossEntropyLoss\"\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"/Users/jan/Documents/Master/Thesis/Code/Models/model3\",             # Directory to save the model and checkpoints\n",
    "#     overwrite_output_dir=True,          # Overwrite the content of the output directory\n",
    "#     num_train_epochs=1,                 # Number of training epochs\n",
    "#     per_device_train_batch_size=32,     # Batch size per device during training\n",
    "#     per_device_eval_batch_size=32,      # Batch size for evaluation\n",
    "#     warmup_steps=10000,                 # Number of warmup steps for learning rate scheduler\n",
    "#     weight_decay=0.01,                  # Strength of weight decay\n",
    "#     logging_dir='./logs',               # Directory for storing logs\n",
    "#     logging_steps=500,                  # Log every 500 steps\n",
    "#     save_steps=10000,                   # Save checkpoint every 10,000 steps\n",
    "#     save_total_limit=20,                # Limit the total amount of checkpoints and deletes the older checkpoints\n",
    "#     learning_rate=5e-5,                 # Learning rate\n",
    "#     gradient_accumulation_steps=2,      # Number of updates steps to accumulate before performing a backward/update pass\n",
    "#     evaluation_strategy='steps',        # Evaluate every `eval_steps`\n",
    "#     eval_steps=5000,                    # Evaluation and save checkpoints every 5,000 steps\n",
    "#     gradient_checkpointing=False,       # Use gradient checkpointing to save memory at the cost of slower backward pass\n",
    "#     fp16=True,                          # Use 16-bit (mixed) precision instead of 32-bit\n",
    "#     report_to='tensorboard',            # Use TensorBoard to log training information\n",
    "#     load_best_model_at_end=True,        # Load the best model when finished training\n",
    "#     metric_for_best_model='accuracy',   # The metric to use to compare models\n",
    "#     greater_is_better=True              # Whether the metric should be maximized\n",
    "# )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meer arguments\n",
    "\n",
    "\n",
    "# training_args2 = TrainingArguments(\n",
    "#     output_dir='./results',               # V\n",
    "#     num_train_epochs=3,                   # V\n",
    "#     per_device_train_batch_size=16,       # wat is hier goed? GPT raadt 16 aan om mee te beginnen\n",
    "#     learning_rate=5e-5,                   # wat is hier goed? ga geen hyp param search doen\n",
    "#     weight_decay=0.01,                    # Weight decay for regularization\n",
    "#     logging_dir='./logs',                 # Directory for logs. Is dit nodig? \n",
    "#     logging_steps=500,                    # Frequency of logging. IS dit nodig?\n",
    "#     save_steps=10_000,                    # Frequency of saving checkpoints\n",
    "#     save_total_limit=2,                   # waarom zou je dit niet hoog zetten? denk dat geheugen geen probleem is\n",
    "#     evaluation_strategy='steps',          # Evaluation strategy ('no', 'steps', 'epoch')\n",
    "#     eval_steps=500,                       # Frequency of evaluation if strategy is 'steps'\n",
    "#     load_best_model_at_end=True,          # Whether to load the best model at the end of training\n",
    "# )\n",
    "\n",
    "\n",
    "# # dit wordt aangeraden door GPT:\n",
    "# training_args3 = TrainingArguments(\n",
    "#     output_dir='./results',               # Directory to save model checkpoints and logs\n",
    "#     num_train_epochs=3,                   # Number of training epochs\n",
    "#     per_device_train_batch_size=16,       # Batch size for training on each device\n",
    "#     per_device_eval_batch_size=16,        # Batch size for evaluation on each device\n",
    "#     learning_rate=1e-4,                   # Initial learning rate\n",
    "#     weight_decay=0.01,                    # Weight decay for regularization\n",
    "#     logging_dir='./logs',                 # Directory for logs\n",
    "#     logging_steps=500,                    # Frequency of logging\n",
    "#     save_steps=5000,                      # Frequency of saving checkpoints\n",
    "#     save_total_limit=2,                   # Maximum number of checkpoints to keep\n",
    "#     evaluation_strategy='steps',          # Evaluation strategy ('no', 'steps', 'epoch')\n",
    "#     eval_steps=5000,                      # Frequency of evaluation if strategy is 'steps'\n",
    "#     load_best_model_at_end=True,          # Whether to load the best model at the end of training\n",
    "#     fp16=True,                            # Enable mixed precision training\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"/Users/jan/Documents/Master/Thesis/model-xx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"/Users/jan/Documents/Master/Thesis/model-xx\",\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.03317979723215103,\n",
       "  'token': 274,\n",
       "  'token_str': ' de',\n",
       "  'sequence': 'Ik ga morgen naar de.'},\n",
       " {'score': 0.027490602806210518,\n",
       "  'token': 287,\n",
       "  'token_str': ' van',\n",
       "  'sequence': 'Ik ga morgen naar van.'},\n",
       " {'score': 0.022509176284074783,\n",
       "  'token': 295,\n",
       "  'token_str': ' een',\n",
       "  'sequence': 'Ik ga morgen naar een.'},\n",
       " {'score': 0.01623355969786644,\n",
       "  'token': 18,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'Ik ga morgen naar..'},\n",
       " {'score': 0.015407562255859375,\n",
       "  'token': 317,\n",
       "  'token_str': ' voor',\n",
       "  'sequence': 'Ik ga morgen naar voor.'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Ik ga morgen naar <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.00024192493583541363,\n",
       "  'token': 4352,\n",
       "  'token_str': ' niemand',\n",
       "  'sequence': 'Ik ga morgen naar niemand.'},\n",
       " {'score': 0.00020695451530627906,\n",
       "  'token': 19344,\n",
       "  'token_str': 'abine',\n",
       "  'sequence': 'Ik ga morgen naarabine.'},\n",
       " {'score': 0.00020639803551603109,\n",
       "  'token': 20510,\n",
       "  'token_str': ' vetten',\n",
       "  'sequence': 'Ik ga morgen naar vetten.'},\n",
       " {'score': 0.00020594659144990146,\n",
       "  'token': 11583,\n",
       "  'token_str': ' wapens',\n",
       "  'sequence': 'Ik ga morgen naar wapens.'},\n",
       " {'score': 0.00020444301480893046,\n",
       "  'token': 26612,\n",
       "  'token_str': 'sules',\n",
       "  'sequence': 'Ik ga morgen naarsules.'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we do this for the untrained model:\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "fill_mask_untrained = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "fill_mask_untrained(\"Ik ga morgen naar <mask>.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the untrained model gives random tokens. Our very minimally trained model (only 100.000 lines of text) already predicts some more probable tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train our model further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model\n",
    "\n",
    "from transformers import AutoConfig\n",
    "\n",
    "model_dir = \"/Users/jan/Documents/Master/Thesis/model-xx\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_dir)\n",
    "\n",
    "model_trained_reloaded = RobertaForMaskedLM.from_pretrained(\n",
    "    model_dir,\n",
    "    config=config,\n",
    "    use_safetensors=True  # Ensure that the model is loaded from safetensors\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test whether the reloaded model returns the same predictions\n",
    "\n",
    "fill_mask_reloaded = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model_trained_reloaded,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "fill_mask_untrained2(\"Ik ga morgen naar <mask>.\") == fill_mask(\"Ik ga morgen naar <mask>.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have succesfully loaded the trained model. We now want to train it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'AcceleratorConfig' on <module 'transformers.trainer_pt_utils' from '/Users/jan/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/transformers/trainer_pt_utils.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/jan/Documents/Master/Thesis/model-xx/training_args.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# reload the tokenizer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RobertaTokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/scur2141/train_tokenizer1/tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/torch/serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[1;32m   1015\u001b[0m                      map_location,\n\u001b[1;32m   1016\u001b[0m                      pickle_module,\n\u001b[1;32m   1017\u001b[0m                      overall_storage\u001b[39m=\u001b[39;49moverall_storage,\n\u001b[1;32m   1018\u001b[0m                      \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m   1019\u001b[0m \u001b[39mif\u001b[39;00m mmap:\n\u001b[1;32m   1020\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmmap can only be used with files saved with \u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/torch/serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1421\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1422\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1424\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1425\u001b[0m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1426\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtorch.load.metadata\u001b[39m\u001b[39m\"\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mserialization_id\u001b[39m\u001b[39m\"\u001b[39m: zip_file\u001b[39m.\u001b[39mserialization_id()}\n\u001b[1;32m   1427\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/torch/serialization.py:1415\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m mod_name \u001b[39m=\u001b[39m load_module_mapping\u001b[39m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1415\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfind_class(mod_name, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'AcceleratorConfig' on <module 'transformers.trainer_pt_utils' from '/Users/jan/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/transformers/trainer_pt_utils.py'>"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# reload the tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"/home/scur2141/train_tokenizer1/tokenizer\", max_len=512)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Users/jan/Documents/Master/Thesis/Code/Models/model2\", # note, this is not where the model is saved, just info about training\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_trained_reloaded, # BE SURE to take the reloaded model here\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_methods(cls):\n",
    "    \"\"\"\n",
    "    This function returns a list of all methods in the given class.\n",
    "    \n",
    "    :param cls: The class to inspect.\n",
    "    :return: A list of method names.\n",
    "    \"\"\"\n",
    "    methods = [method_name for method_name in dir(cls) \n",
    "               if callable(getattr(cls, method_name))]\n",
    "    return methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/Short/OSCAR_short.txt') as input:\n",
    "    with open('/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/Short/OSCAR_shorter2.txt', 'w') as output:\n",
    "        for i, line in enumerate(input):\n",
    "\n",
    "            if i > 3000 and i < 4000:\n",
    "                output.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d88b4992e5b06b322ead3277a6c0c22aea73de159b15016e3d5eb1aecc84f355"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
