{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Training pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file the training pipeline that we used is displayed. This notebook was not actually used for training, as this was all done on the snellius computer. Python scripts, based on this notebook, were used for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare files and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to change when working in different environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk, Dataset\n",
    "import os\n",
    "from transformers import AutoTokenizer, RobertaTokenizerFast\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import pipeline\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "\n",
    "# general function\n",
    "def load_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        my_dict = json.load(f)\n",
    "    return my_dict\n",
    "\n",
    "# data files\n",
    "path = '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/Short/OSCAR_short.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available? False\n"
     ]
    }
   ],
   "source": [
    "# Check that PyTorch sees it\n",
    "print(f'Cuda available? {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_text_generator(gen):\n",
    "#     for i in gen:\n",
    "#         yield i['text']\n",
    "\n",
    "\n",
    "\n",
    "# oscar_short = load_dataset('text', data_files={\"train\": path}, split='train')\n",
    "# oscar_short_it = load_dataset('text', data_files={\"train\": path}, split='train', streaming=True)\n",
    "\n",
    "# p = '/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/nl_part_1.txt'\n",
    "\n",
    "# oscar1_it = load_dataset('text', data_files={\"train\": p}, split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # functions for data\n",
    "# def load_datasets_from_paths(paths):\n",
    "#     datasets_list = []\n",
    "\n",
    "#     # Load each dataset from the provided paths\n",
    "#     for path in paths:\n",
    "#         dataset = load_dataset('text', data_files={\"train\": path}, split='train', streaming=True)\n",
    "#         datasets_list.append(dataset)\n",
    "    \n",
    "#     return datasets_list\n",
    "\n",
    "# def combine_and_batch_datasets(datasets, batch_size=1000):\n",
    "#     \"\"\"Yield batches of items from a list of iterable datasets.\"\"\"\n",
    "#     for dataset in datasets:\n",
    "#         batch = []\n",
    "#         for item in dataset:\n",
    "#             batch.append(item['text'])\n",
    "#             if len(batch) == batch_size:\n",
    "#                 yield batch\n",
    "#                 batch = []\n",
    "#         # Yield any remaining items in the current dataset that didn't complete a full batch\n",
    "#         if batch:\n",
    "#             yield batch\n",
    "\n",
    "# #paths = [str(x) for x in Path('/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR').glob(\"**/*.txt\")]\n",
    "\n",
    "\n",
    "# #combined_generator = combine_and_batch_datasets(load_datasets_from_paths(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = ['/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/lower/nl_part_1_lower_short.txt']\n",
    "\n",
    "# combined_generator1 = combine_and_batch_datasets(load_datasets_from_paths(p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/jan/Documents/Master/Thesis/Code/Tokenizers/BPE_NEW/vocab.json',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/Tokenizers/BPE_NEW/merges.txt']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# paths = ['/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/lower/nl_part_1_lower_short.txt']\n",
    "\n",
    "# # Initialize a tokenizer\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# # Customize training\n",
    "# tokenizer.train(files=paths, vocab_size=50023, min_frequency=2, special_tokens=[\n",
    "#     \"<s>\",\n",
    "#     \"<pad>\",\n",
    "#     \"</s>\",\n",
    "#     \"<unk>\",\n",
    "#     \"<mask>\",\n",
    "# ])\n",
    "\n",
    "# tokenizer.save_model(\"/Users/jan/Documents/Master/Thesis/Code/Tokenizers/BPE_NEW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/Users/jan/Documents/Master/Thesis/Code/Tokenizers/WPX/tokenizer_config.json',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/Tokenizers/WPX/special_tokens_map.json',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/Tokenizers/WPX/vocab.txt',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/Tokenizers/WPX/added_tokens.json',\n",
       " '/Users/jan/Documents/Master/Thesis/Code/Tokenizers/WPX/tokenizer.json')"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # set vocabulary size\n",
    "# vocab_size = 50023\n",
    "\n",
    "# # select dataset to train with\n",
    "# train_set = combined_generator1\n",
    "# # train_set = create_text_generator(oscar_short_it)\n",
    "\n",
    "# # load an existing tokenizer\n",
    "# old_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # retrain the tokenizer\n",
    "# tokenizerX = old_tokenizer.train_new_from_iterator(train_set, vocab_size)\n",
    "\n",
    "# tokenizerX.save_pretrained('/Users/jan/Documents/Master/Thesis/Code/Tokenizers/WPX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50005"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizerBPE = RobertaTokenizerFast.from_pretrained(\"/Users/jan/Documents/Master/Thesis/Code/Tokenizers/BPE_NEW\", max_len=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_wp = AutoTokenizer.from_pretrained('/Users/jan/Documents/Master/Thesis/Code/Tokenizers/WPX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 3: Custom morphological tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomTokenizer:\n",
    "\n",
    "    def __init__(self, segmentation_dictionary, wp_tokenizer, max_length=None, pad_to_multiple_of=None, model_max_length=None):\n",
    "\n",
    "\n",
    "\n",
    "        self.wp_tokenizer = wp_tokenizer\n",
    "        self.wp_vocab = self.wp_tokenizer.get_vocab()\n",
    "\n",
    "        self.segmentations = {word: seg for word, seg in segmentation_dictionary.items() if len(seg) > 0}\n",
    "        self.seg_dict = {}\n",
    "\n",
    "        \n",
    "        for word, segs in self.segmentations.items():\n",
    "            out = []\n",
    "            for i, seg in enumerate(segs):\n",
    "                if i == 0:\n",
    "                    out.append(seg)\n",
    "                else:\n",
    "                    out.append('##' + seg)\n",
    "            self.seg_dict[word] = out\n",
    "        \n",
    "\n",
    "        self.segments = {seg for segs in self.seg_dict.values() for seg in segs}\n",
    "\n",
    "        self.vocab = self.wp_vocab.copy()\n",
    "        \n",
    "      \n",
    "\n",
    "        next_index = len(self.vocab)\n",
    "        \n",
    "        for element in self.segments:\n",
    "            if element not in self.vocab:\n",
    "                self.vocab[element] = next_index\n",
    "                next_index += 1\n",
    "\n",
    "\n",
    "        \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        self.seg_dict_numbered = {}\n",
    "\n",
    "        for word, segs in self.seg_dict.items():\n",
    "            s = []\n",
    "            for seg in segs:\n",
    "                s.append(self.vocab[seg])\n",
    "            self.seg_dict_numbered[word] = s\n",
    "        \n",
    "        \n",
    "        self.inverted_vocab = {value: key for key, value in self.vocab.items()}\n",
    "        self.max_length = max_length\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "        self.padding_side = 'right'\n",
    "        self.model_max_length = model_max_length\n",
    "\n",
    "        ### special tokens\n",
    "\n",
    "        special_tokens = ['[UNK]', '[MASK]', '[CLS]', '[SEP]', '[PAD]']\n",
    "        special_token_ids = {}\n",
    "        \n",
    "        for token in special_tokens:\n",
    "            if token not in self.vocab:\n",
    "                self.vocab[token] = len(self.vocab)\n",
    "            special_token_ids[token] = self.vocab[token]\n",
    "        \n",
    "        self.unk_token = '[UNK]'\n",
    "        self.unk_token_id = special_token_ids['[UNK]']\n",
    "        \n",
    "        self.mask_token = '[MASK]'\n",
    "        self.mask_token_id = special_token_ids['[MASK]']\n",
    "    \n",
    "        \n",
    "        self.pad_token = '[PAD]'\n",
    "        self.pad_token_id = special_token_ids['[PAD]']\n",
    "        \n",
    "        self.bos_token = '[CLS]'\n",
    "        self.bos_token_id = special_token_ids['[CLS]']\n",
    "        \n",
    "        self.eos_token = '[SEP]'\n",
    "        self.eos_token_id = special_token_ids['[SEP]']\n",
    "        \n",
    "\n",
    "        self.special_tokens = [self.vocab['[PAD]'], self.vocab['[UNK]'], self.vocab['[CLS]'], self.vocab['[SEP]'], self.vocab['[MASK]']]\n",
    "\n",
    "        self.special_tokens_map = wp_tokenizer.special_tokens_map\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "\n",
    "    def check_tokens_in_dict(self, ids, tokens, dic_a):\n",
    "     \n",
    "        combined_tokens = []\n",
    "        current_word = ''\n",
    "        current_ids = []\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token.startswith('##'):\n",
    "                current_word += token[2:]\n",
    "                current_ids.append(ids[i])\n",
    "            else:\n",
    "                if current_word:\n",
    "                    combined_tokens.append((current_word, current_ids))\n",
    "                current_word = token\n",
    "                current_ids = [ids[i]]\n",
    "\n",
    "        if current_word:\n",
    "            combined_tokens.append((current_word, current_ids))\n",
    " \n",
    "        result = []\n",
    "        for word, ids_list in combined_tokens:\n",
    "            if word in dic_a:\n",
    "                result.extend(dic_a[word])  \n",
    "            else:\n",
    "                result.extend(ids_list)\n",
    "        return result\n",
    "    \n",
    "\n",
    "    def check_tokens_in_dict_v2(self, ids, tokens, dic_a):\n",
    "\n",
    "        combined_tokens = []\n",
    "        current_word = ''\n",
    "        current_tokens = []\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token.startswith('##'):\n",
    "                current_word += token[2:]\n",
    "                current_tokens.append(token)\n",
    "            else:\n",
    "                if current_word:\n",
    "                    combined_tokens.append((current_word, current_tokens))\n",
    "        \n",
    "                current_word = token\n",
    "                current_tokens = [token]\n",
    "\n",
    "        if current_word:\n",
    "            combined_tokens.append((current_word, current_tokens))\n",
    "    \n",
    "        result = []\n",
    "        for word, tokens_list in combined_tokens:\n",
    "            if word in dic_a:\n",
    "                result.extend(dic_a[word])  \n",
    "            else:\n",
    "                result.extend(tokens_list)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        if token in self.vocab:\n",
    "            return self.vocab[token]\n",
    "        else:\n",
    "            return self.unk_token_id\n",
    "\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        if isinstance(tokens, list):\n",
    "            return [self._convert_token_to_id(token) for token in tokens]\n",
    "        return self._convert_token_to_id(tokens)\n",
    "\n",
    "\n",
    "\n",
    "    def _convert_id_to_token(self, id):\n",
    "        return self.inverted_vocab[id]\n",
    "\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        if isinstance(ids, list):\n",
    "            return [self._convert_id_to_token(id) for id in ids]\n",
    "        return self._convert_id_to_token(ids)\n",
    "\n",
    "\n",
    "    def get_special_tokens_mask(self, token_ids, already_has_special_tokens=False):\n",
    "        return [1 if self._is_special_token(token_id) else 0 for token_id in token_ids]\n",
    "\n",
    "\n",
    "    def _is_special_token(self, token_id):\n",
    "\n",
    "        if token_id in self.special_tokens:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    \n",
    "\n",
    "    def pad(self, batch, return_tensors=\"pt\", pad_to_multiple_of=None, padding=True, max_length=None):\n",
    "        if pad_to_multiple_of is None:\n",
    "            pad_to_multiple_of = self.pad_to_multiple_of\n",
    "\n",
    "        input_ids_list = []\n",
    "        for dictionary in batch:\n",
    "            for key, value in dictionary.items():\n",
    "                if key == \"input_ids\":\n",
    "                    \n",
    "                    if isinstance(value, torch.Tensor):\n",
    "                        input_ids_list.append(value.tolist())\n",
    "                    else:\n",
    "                        input_ids_list.append(value)\n",
    "\n",
    "\n",
    "        max_length = max(len(x) for x in input_ids_list)\n",
    "        \n",
    "        if pad_to_multiple_of is not None:\n",
    "            max_length = (max_length + pad_to_multiple_of - 1) // pad_to_multiple_of * pad_to_multiple_of\n",
    "        \n",
    "        padded_batch = []\n",
    "        for seq in input_ids_list:\n",
    "            if len(seq) < max_length:\n",
    "                seq.extend([self.pad_token_id] * (max_length - len(seq)))\n",
    "            padded_batch.append(seq)\n",
    "        \n",
    "        attention_list = []\n",
    "        for inner_list in padded_batch:\n",
    "            p_list = [1 if value != self.pad_token_id else 0 for value in inner_list]\n",
    "            attention_list.append(p_list)\n",
    "        \n",
    "        if return_tensors == \"pt\":\n",
    "            return {'input_ids': torch.tensor(padded_batch, dtype=torch.long), 'attention_mask': torch.tensor(attention_list, dtype=torch.long)}\n",
    "        \n",
    "        return {'input_ids': padded_batch, 'attention_mask': attention_list}\n",
    "\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        if isinstance(text, list):\n",
    "            tokens_list = []\n",
    "            for t in text:\n",
    "                encoded = self.wp_tokenizer(t)\n",
    "                tokens = encoded.tokens()\n",
    "                tokens = self.check_tokens_in_dict_v2(encoded['input_ids'], tokens, self.seg_dict)[1:-1]\n",
    "                tokens_list.append(tokens)\n",
    "            return tokens_list\n",
    "        else:\n",
    "            encoded = self.wp_tokenizer(text)\n",
    "            tokens = encoded.tokens()\n",
    "            tokens = self.check_tokens_in_dict_v2(encoded['input_ids'], tokens, self.seg_dict)[1:-1]\n",
    "            return tokens\n",
    "\n",
    "\n",
    "    def encode(self, text, text_pair=None, add_special_tokens=True, return_tensors=None, max_length=None, pad_to_max_length=False, truncation=False):\n",
    "        if text_pair:\n",
    "            text = f\"{text} {self.eos_token} {text_pair}\"\n",
    "\n",
    "        tokens = self.tokenize(text)\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            tokens = [self.bos_token] + tokens + [self.eos_token]\n",
    "        \n",
    "        if truncation and max_length and len(tokens) > max_length:\n",
    "            tokens = tokens[:max_length]\n",
    "        \n",
    "        if pad_to_max_length and max_length and len(tokens) < max_length:\n",
    "            tokens += [self.pad_token] * (max_length - len(tokens))\n",
    "        \n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1 if token != self.pad_token else 0 for token in tokens]\n",
    "\n",
    "        if return_tensors == \"pt\":\n",
    "            input_ids = torch.tensor([input_ids], dtype=torch.long)\n",
    "            attention_mask = torch.tensor([attention_mask], dtype=torch.long)\n",
    "        \n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "    def batch_encode_plus(self, texts, text_pairs=None, add_special_tokens=True, return_tensors=None, max_length=None, pad_to_max_length=False, truncation=False, pad_to_multiple_of=None):\n",
    "        batch = []\n",
    "\n",
    "        if text_pairs:\n",
    "            for text, text_pair in zip(texts, text_pairs):\n",
    "                batch.append(self.encode(\n",
    "                    text, \n",
    "                    text_pair=text_pair, \n",
    "                    add_special_tokens=add_special_tokens, \n",
    "                    return_tensors=None, \n",
    "                    max_length=max_length, \n",
    "                    pad_to_max_length=pad_to_max_length, \n",
    "                    truncation=truncation\n",
    "                ))\n",
    "        else:\n",
    "            for text in texts:\n",
    "                batch.append(self.encode(\n",
    "                    text, \n",
    "                    add_special_tokens=add_special_tokens, \n",
    "                    return_tensors=None, \n",
    "                    max_length=max_length, \n",
    "                    pad_to_max_length=pad_to_max_length, \n",
    "                    truncation=truncation\n",
    "                ))\n",
    "\n",
    "        padded_batch = self.pad(batch, return_tensors=return_tensors, pad_to_multiple_of=pad_to_multiple_of)\n",
    "\n",
    "        return padded_batch\n",
    "\n",
    "\n",
    "    def decode(self, ids, skip_special_tokens=False):\n",
    "        out = ''\n",
    "        for id in ids:\n",
    "            token = self._convert_id_to_token(id)\n",
    "            if skip_special_tokens and self._is_special_token(id):\n",
    "                continue\n",
    "            if token[:2] == '##':\n",
    "                out += token[2:]\n",
    "            else:\n",
    "                out += ' ' + token\n",
    "        \n",
    "        \n",
    "        out = re.sub(r'\\s+([?.!,\\'\"])', r'\\1', out)\n",
    "        return out.strip()\n",
    "\n",
    "\n",
    "    def __call__(self, text, text_pair=None, add_special_tokens=True, return_tensors=None, max_length=None, pad_to_max_length=False, truncation=False, pad_to_multiple_of=None):\n",
    "        if isinstance(text, str):\n",
    "      \n",
    "            return self.encode(\n",
    "                text, \n",
    "                text_pair=text_pair, \n",
    "                add_special_tokens=add_special_tokens, \n",
    "                return_tensors=return_tensors, \n",
    "                max_length=max_length, \n",
    "                pad_to_max_length=pad_to_max_length, \n",
    "                truncation=truncation\n",
    "            )\n",
    "        elif isinstance(text, list) and all(isinstance(t, str) for t in text):\n",
    "      \n",
    "            return self.batch_encode_plus(\n",
    "                text, \n",
    "                text_pairs=text_pair, \n",
    "                add_special_tokens=add_special_tokens, \n",
    "                return_tensors=return_tensors, \n",
    "                max_length=max_length, \n",
    "                pad_to_max_length=pad_to_max_length, \n",
    "                truncation=truncation, \n",
    "                pad_to_multiple_of=pad_to_multiple_of\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Input text should be either a single string or a list of strings.\")\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dict = '/Users/jan/Documents/Master/Thesis/Code/Tokenizers/segmentation_dictionary_final.json'\n",
    "path_to_tokenizer = \"/Users/jan/Documents/Master/Thesis/Code/Tokenizers/HELP/MORF/help_2815\"\n",
    "\n",
    "segmentation_dictionary = load_json(path_to_dict)\n",
    "help_tokenizer = AutoTokenizer.from_pretrained(path_to_tokenizer)\n",
    "\n",
    "tokenizer_morf = CustomTokenizer(segmentation_dictionary, help_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our training pipeline, we will not tokenize the data during training. Instead, we first create a Dataset object with the tokenized training data. This ensures that the speed of the tokenizer is nat a bottleneck when training. This could especially be the case for the custom tokenizer, which is a bit slower than the two other ones, although not by that much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# pick tokenizer\n",
    "tokenizer = tokenizer_bpe\n",
    "\n",
    "# pick text file to transform\n",
    "text_file_paths = ['/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/Short/OSCAR_shorter2.txt']\n",
    "\n",
    "#text_file_paths = ['/Users/jan/Documents/Master/Thesis/Code/Datasets/OSCAR/lower/nl_part_1_lower_short.txt']\n",
    "\n",
    "\n",
    "# def tokenize function\n",
    "def tokenize_line(line):\n",
    "    return {'input_ids': tokenizer(line.strip(), add_special_tokens=True, truncation=True, max_length=512)['input_ids']}\n",
    "\n",
    "\n",
    "# read the text file and tokenize each line\n",
    "\n",
    "for text_file_path in text_file_paths:\n",
    "    with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    tokenized_lines = [tokenize_line(line) for line in lines]\n",
    "\n",
    "    dataset = Dataset.from_list(tokenized_lines).with_format(\"torch\")\n",
    "    base_name = text_file_path.strip('.txt')\n",
    "    base_name = os.path.splitext(os.path.basename(text_file_path))[0]\n",
    "    dataset_name = f'{base_name}_tokenized'\n",
    "\n",
    "    p = f'/Users/jan/Documents/Master/Thesis/Code/Datasets/Tokenized/TEST/{dataset_name}'\n",
    "    print(dataset_name)\n",
    "\n",
    "    #save\n",
    "    #dataset.save_to_disk(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what a single element in the dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,    57,  1346,   656,   470,   299,  2452,   557,   576,   314,\n",
       "           778,    18,   225,    56,   476,   361,   381,   312,  1233,    16,\n",
       "           518,   361,   557,  3312,   609,   379, 42336,     2])}"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that no element is longer than the max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 512\n",
      "at nr 255\n"
     ]
    }
   ],
   "source": [
    "def test_length(dataset):\n",
    "    mx = 0\n",
    "    for i in range(999):\n",
    "        if dataset[i]['input_ids'].size()[0] > mx:\n",
    "            mx = dataset[i]['input_ids'].size()[0]\n",
    "            n = i\n",
    "    print('max length:', mx)\n",
    "    print('at nr', n)\n",
    "\n",
    "test_length(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether the special tokens still get added correctly for the truncated elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,    52,   317,  1414,    77, 17104,  2356, 12250,  2110,   423,\n",
       "           300,  3568,   563,  1049,   396,   842,  3752,    16,   225,    55,\n",
       "          4489, 20237,    69,  6267,    16,   225,    55,  1532,  5455,   423,\n",
       "          3733, 41985,    16,   225,    43,   302,  2110,   423,   225,    56,\n",
       "            58,   443,  2754,   563,   301,   757,    83,   225,    45,    82,\n",
       "           458,  1059,    69,    16,   225,    42, 10755,  1680,   423,   225,\n",
       "            38,    45,    51,    55,    17,    69,    16,   225,    47,    83,\n",
       "         12733,   225,    38,  1630,  1625,  3239,  4437,   459,   408, 23508,\n",
       "           225,    51,   329,   842,   580,  1054, 23523, 17104,   301,   580,\n",
       "           267, 38950,    16,   225,    38,   367,  6930,   666,   225,    45,\n",
       "          2957,    16,   225,    55,    18,    56,    18,    37,    18,    48,\n",
       "            18,    47,    18,    41,    18,    54,  8021,   225,    55, 12168,\n",
       "           914,   380,   225,    39, 33334,  2173,  3585,    16,   225,    55,\n",
       "            81,  3716, 20237,   666,   633,  4238,    16,   225,    51,   329,\n",
       "           842,  3752,   225,    56,   344, 27300,  1049, 10744, 30530,   919,\n",
       "            94,  4988, 17104,   396,   842,  3752,    16,   225,    47,   413,\n",
       "            83, 11751,   344,    90,  3417,  3733,   271,    89, 23508,   225,\n",
       "            56,   385,  1035,  6374,    22, 13332,   305,    16,   225,    55,\n",
       "            18,    56,    18,    37,    18,    48,    18,    47,    18,    41,\n",
       "            18,    54,  8021,   225,    39,   862,   380,   225,    52,   930,\n",
       "            93,   286,    16,   711,   225,    43,    44,    94,   225,    51,\n",
       "            39,   225,    39, 21539,    16,   225,    57,    84,  1504,    30,\n",
       "           225,    45,    82,  3923,   572, 41985,   225,    59,   266,   225,\n",
       "            58,  3568,  5608,    77,  1007,   300,   225,    57,    55,    38,\n",
       "          8893,    69,    16,   225,    59,   508,  4175,  1007, 17438,   275,\n",
       "           271, 18448,  1049, 44197, 42909,    16,   225,    47,   413,    83,\n",
       "          1128, 32789,  3417, 41753,   312, 13950,    89, 23508,   225,    58,\n",
       "           533,    77, 27300, 17104, 17273,   618,   623,  4655,    77, 27300,\n",
       "           460,  9528,  1142,    16,   283,    38,  1208,   526, 10744, 30530,\n",
       "          1049, 17273,    75,   906, 21566,    16,   225,    48,  9604,  2476,\n",
       "         12755,  3761,  2829, 44426,    16,   225,    50, 16612,  8424,   423,\n",
       "          2476, 12755,   666, 13332,   305,    16,   225,    52, 17062,   275,\n",
       "         40568,  2537,   275, 13332,   305,    16,   225,    51,   329,   842,\n",
       "           580,  1054,   225,    37,    49,    40,  3050,  7075,   399,   550,\n",
       "           318,    16, 11883,   302,    16, 45227,    16, 32306,  1195,   225,\n",
       "            52,   385,   805,    77,    16,  4231,    71,  3716,  1049,  1321,\n",
       "          2707, 17582,  4539, 17104, 13332,   305,    16,   225,    51,  4134,\n",
       "            78,  2122, 45242,   263,   846,  1625,   264,   312, 21254,    83,\n",
       "            78,   427,  5317, 12733, 23508,   225,    47,   413,    83,  3600,\n",
       "          2670,   318,   424,   788,    84,    16,    94,   618,   275,   762,\n",
       "           806,  6244,  2700,    94,   630,    89,    16,   225,    47,    83,\n",
       "         15301,  1706,   271,    89,  3505,   491,  3528,   620,   286,   408,\n",
       "         23508,   225,    56,    73,   275,  3716,   285,   762,    78,   543,\n",
       "            16,   225,    59,   508,  4175,  1007,  1372,  8816,    16,   225,\n",
       "            57,    80, 10932,   286,   582,  1028,   314,   563, 17104, 17273,\n",
       "           618,   623, 13172,   261,   549,    82,  7977,    16,   225,    47,\n",
       "           413,    83, 21813,    78,  3417,   285, 36874, 27300, 38950, 17273,\n",
       "          5696, 27300,  3761,   289,   922,   576,   225,    52,    39,  5417,\n",
       "            16,   225,    39, 10824,    42,  1232,   225,    58,   533,    77,\n",
       "         27300,    16,   225,    50, 43334,  5455,  2227, 17104,  7243, 20237,\n",
       "            89,  1090,  9861,   572,   271,    89,   399,    52,    10,    51,\n",
       "          1195,     2])}"
      ]
     },
     "execution_count": 819,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[255]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the dataset works in combination with the data collator we use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_collator(batch):\n",
    "    input_ids = batch['input_ids']\n",
    "    labels = batch['labels']\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # check masked elements\n",
    "    mask_check = True\n",
    "    for i in range(input_ids.shape[0]):\n",
    "        for j in range(input_ids.shape[1]):\n",
    "            if labels[i][j] != -100:\n",
    "                if input_ids[i][j] != labels[i][j]:\n",
    "                    mask_check = False\n",
    "                    print(f\"Mismatch at position [{i}, {j}]: input_id={input_ids[i][j]}, label={labels[i][j]}\")\n",
    "\n",
    "    # check unmasked elements\n",
    "    unmask_check = True\n",
    "    for i in range(input_ids.shape[0]):\n",
    "        for j in range(input_ids.shape[1]):\n",
    "            if labels[i][j] == -100:\n",
    "                if input_ids[i][j] == labels[i][j]:\n",
    "                    unmask_check = False\n",
    "                    print(f\"Unexpected match at position [{i}, {j}]: input_id={input_ids[i][j]}, label={labels[i][j]}\")\n",
    "\n",
    "    # check padding\n",
    "    padding_check = True\n",
    "    for i in range(input_ids.shape[0]):\n",
    "        for j in range(input_ids.shape[1]):\n",
    "            if input_ids[i][j] == pad_token_id:\n",
    "                if labels[i][j] != -100:\n",
    "                    padding_check = False\n",
    "                    print(f\"Padding mismatch at position [{i}, {j}]: input_id={input_ids[i][j]}, label={labels[i][j]}\")\n",
    "\n",
    "    if mask_check and unmask_check and padding_check:\n",
    "        print(\"All checks passed.\")\n",
    "    else:\n",
    "        print(\"There were some mismatches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([    0,  1830,   225,    45,    82,  1317,   287,   272,   225,    44,\n",
       "            334,   225,    46,    73,  4935,   225,    39,  7156,   459,   398,\n",
       "           3438,   503,  3993,   830, 14758,   310,  4174,   661,   609,   314,\n",
       "            798,   288,   661,   674,  3979,   314,  6121,    18,     2])},\n",
       " {'input_ids': tensor([    0,    59,    73,   360,  1603,   470,   428,   272,  3750,   287,\n",
       "            272,  2334,   372, 27359,  1262,   293,   557,  1008,  6888,   360,\n",
       "             16,  1766,   830,   622,   412,  3423, 18796,   396,  1204,   337,\n",
       "           1397,   524,    16,   288,   674,  3734,  1990,   985,   288,  1539,\n",
       "           7585,   288, 15432,   361,   677, 47249, 41961,  1535,    18,     2])},\n",
       " {'input_ids': tensor([    0,    46,    73,  4935,  5872,  1214,   411,   299,  1008,    18,\n",
       "            225,    44,   271, 11025,   940,   272, 15666,  1037,  5067,  6116,\n",
       "             16,   288,   427,  3110,   225,    44,   437,   293,   272,  2847,\n",
       "            314, 24603,    18,   225,    41, 29270,   790,   272,  1333,   380,\n",
       "            368,   484,  3620,  4170,  1075,   454,   337,   272,  9976,    16,\n",
       "            288,   677,   418,    30,     2])},\n",
       " {'input_ids': tensor([    0,  4813,   714,    49,   335,   680,    16,   225,    49,  6966,\n",
       "             87,   478,   557,   299,  1299, 13841,    30,   405,   295, 19449,\n",
       "            673, 26924,   959,   351,   360,  1046,  1153,   478,   403,   657,\n",
       "             16,   694,   360,  5253,   330,   372,  1046, 12261,   288, 30707,\n",
       "          35585,   316,   360,  5253,    18,     2])},\n",
       " {'input_ids': tensor([    0,  5292,   288,  2302,   272,  2022,    16,   288,  1285,   272,\n",
       "            637, 10443,    16,   436,   451,  4016,  1233,   427,   884,  3259,\n",
       "           1285,   427, 14462,   357,    18,     2])},\n",
       " {'input_ids': tensor([    0,  4656,   436,  1471, 18310, 16665,   326,   952,   314,  1309,\n",
       "            337,   272,  3799,  1112,   288,   337,   272, 38543,   287,   272,\n",
       "          10328,    16,  3120,    88,   379,   288,   492,   379,  1348, 33330,\n",
       "             88,    18,     2])}]"
      ]
     },
     "execution_count": 822,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [dataset[0], dataset[1], dataset[2], dataset[3]]\n",
    "texts = [dataset[4], dataset[5], dataset[6], dataset[7], dataset[8], dataset[9]]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1830,   225,    45,    82,  1317,   287,   272,     4,    44,\n",
       "           334,   225,     4,    73,  4935,   225,     4,     4,   459,   398,\n",
       "          3438,     4,  3993,     4, 14758,   310,  4174,   661,   609,   314,\n",
       "           798,   288,   661,   674,     4,   314,  6121,    18,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1],\n",
       "        [    0,    59,    73,   360,  1603,     4,   428,   272,  3750, 12178,\n",
       "           272,  2334,   372,     4,  1262,   293,   557,  1008,  6888,   360,\n",
       "            16,  1766,     4,   622,   412,     4, 18796,   396,  1204,   337,\n",
       "          1397,     4,    16,     4,   674,  3734,  1990,   985,   288,     4,\n",
       "          7585,   288, 15432,   361,   677, 47249, 41961,  1535,    18,     2,\n",
       "             1,     1,     1,     1,     1],\n",
       "        [    0,    46,    73,  4935,  5872,  1214,   411,   299,  1008,    18,\n",
       "           225,    44,     4, 11025,   940,   272, 15666,     4,  5067,  6116,\n",
       "            16,   288,   427,  3110,   225,    44,   437,   293,   272,  2847,\n",
       "           314, 24603,    18,   225,    41, 29270,   790,   272,  1333,     4,\n",
       "           368,   484,  3620,     4,  1075,     4,   337,   272,  9976,    16,\n",
       "           288,     4,   418,    30,     2],\n",
       "        [    0,  4813,   714,    49,   335,   680,    16,   225,    49,  6966,\n",
       "            87,     4,   557,   299,  1299, 13841,    30,   405,   295, 19449,\n",
       "           673, 26924,   959,   351,   360,  1046,  1153,   478,   403,   657,\n",
       "            16,     4,   360,  5253, 19159,   372,  1046, 12261,   288, 30707,\n",
       "         35585,     4,   360,  5253,     4,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1],\n",
       "        [    0,  5292,   288,  2302,     4,  2022,    16, 31572,  1285,   272,\n",
       "           637, 10443,    16,   436,   451,  4016,  1233,   427,   884,  3259,\n",
       "          1285,   427, 14462,   357,    18,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1],\n",
       "        [    0,  4656,     4,  1471, 18310, 16665,   326,   952,   314,  1309,\n",
       "           337,   272,  3799,  1112,  9480,   337,   272, 38543,   287,   272,\n",
       "         10328,    16,  3120,     4,   379,   288,   492,     4,  1348, 33330,\n",
       "            88,     4,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   225,  -100,\n",
       "           334,  -100,    46,  -100,  -100,  -100,    39,  7156,  -100,  -100,\n",
       "          -100,   503,  -100,   830,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  3979,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100],\n",
       "        [ -100,  -100,  -100,  -100,  1603,   470,  -100,  -100,  -100,   287,\n",
       "          -100,  -100,  -100, 27359,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,   830,  -100,  -100,  3423, 18796,  -100,  -100,  -100,\n",
       "          -100,   524,  -100,   288,  -100,  -100,  -100,  -100,  -100,  1539,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,   271,  -100,  -100,  -100,  -100,  1037,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   380,\n",
       "          -100,  -100,  -100,  4170,  -100,   454,  -100,  -100,  -100,  -100,\n",
       "          -100,   677,  -100,  -100,  -100],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,   478,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,   694,  -100,  -100,   330,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,   316,  -100,  -100,    18,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100],\n",
       "        [ -100,  -100,  -100,  -100,   272,  -100,  -100,   288,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100],\n",
       "        [ -100,  -100,   436,  -100,  -100,  -100,  -100,   952,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,   288,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,    88,  -100,  -100,  -100,   379,  -100,  -100,\n",
       "          -100,    18,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100]])}"
      ]
     },
     "execution_count": 823,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = data_collator(texts)\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at position [0, 6]: input_id=4, label=2045\n",
      "Mismatch at position [0, 11]: input_id=4, label=4686\n",
      "Mismatch at position [0, 16]: input_id=4, label=1457\n",
      "Mismatch at position [0, 25]: input_id=26359, label=7138\n",
      "Mismatch at position [1, 2]: input_id=9435, label=1192\n",
      "Mismatch at position [1, 19]: input_id=4, label=16\n",
      "Mismatch at position [1, 22]: input_id=4, label=1458\n",
      "Mismatch at position [1, 25]: input_id=4, label=27854\n",
      "Mismatch at position [1, 27]: input_id=4, label=1953\n",
      "Mismatch at position [1, 30]: input_id=4, label=1168\n",
      "Mismatch at position [1, 45]: input_id=4, label=1267\n",
      "Mismatch at position [1, 49]: input_id=4, label=18\n",
      "Mismatch at position [2, 5]: input_id=4, label=1135\n",
      "Mismatch at position [2, 6]: input_id=4, label=1784\n",
      "Mismatch at position [2, 9]: input_id=4, label=16508\n",
      "Mismatch at position [2, 12]: input_id=4, label=16159\n",
      "Mismatch at position [2, 16]: input_id=4, label=16\n",
      "Mismatch at position [2, 31]: input_id=4, label=1206\n",
      "Mismatch at position [3, 7]: input_id=4, label=1386\n",
      "Mismatch at position [3, 32]: input_id=4, label=1200\n",
      "Mismatch at position [3, 34]: input_id=4, label=7491\n",
      "Mismatch at position [3, 38]: input_id=4, label=23250\n",
      "Mismatch at position [4, 14]: input_id=4, label=1273\n",
      "Mismatch at position [4, 19]: input_id=4, label=3950\n",
      "Mismatch at position [5, 3]: input_id=4, label=2171\n",
      "Mismatch at position [5, 13]: input_id=4, label=1959\n",
      "Mismatch at position [5, 23]: input_id=4, label=3713\n",
      "There were some mismatches.\n"
     ]
    }
   ],
   "source": [
    "check_data_collator(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These mismatches are all supposed to be there, they are the cosequence of the randomness in the collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_bpe\n",
    "t_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters model: 43011431\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=t_size,\n",
    "    max_position_embeddings=512,\n",
    "    num_attention_heads=8,\n",
    "    num_hidden_layers=8,\n",
    "    type_vocab_size=1, \n",
    "    hidden_size=512,\n",
    "    intermediate_size=1024,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    mask_token_id= tokenizer.mask_token_id\n",
    ")\n",
    "\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "print(f'number of parameters model: {model.num_parameters()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 512,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1024,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"mask_token_id\": 4,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 8,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.33.3\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50023\n",
       "}"
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Users/jan/Documents/Master/Thesis/Code/Models/model1\", # note: this is not where the model is saved, just info about training\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=1000,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% time\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"/Users/jan/Documents/Master/Thesis/Models/model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.000244285591179505,\n",
       "  'token': 11434,\n",
       "  'token_str': '##korting',\n",
       "  'sequence': 'ik ga morgenkorting'},\n",
       " {'score': 0.00023400774807669222,\n",
       "  'token': 5145,\n",
       "  'token_str': 'box',\n",
       "  'sequence': 'ik ga morgen box'},\n",
       " {'score': 0.0001880083145806566,\n",
       "  'token': 1015,\n",
       "  'token_str': '##',\n",
       "  'sequence': 'ik ga morgen'},\n",
       " {'score': 0.00016760850849095732,\n",
       "  'token': 16143,\n",
       "  'token_str': '##hangende',\n",
       "  'sequence': 'ik ga morgenhangende'},\n",
       " {'score': 0.00015900007565505803,\n",
       "  'token': 24872,\n",
       "  'token_str': 'avail',\n",
       "  'sequence': 'ik ga morgen avail'}]"
      ]
     },
     "execution_count": 760,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask('ik ga morgen [MASK]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the untrained model gives random tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train our model further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload model\n",
    "\n",
    "from transformers import AutoConfig\n",
    "\n",
    "model_dir = \"/Users/jan/Documents/Master/Thesis/model-xx\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_dir)\n",
    "\n",
    "model_trained_reloaded = RobertaForMaskedLM.from_pretrained(\n",
    "    model_dir,\n",
    "    config=config,\n",
    "    use_safetensors=True  # Ensure that the model is loaded from safetensors\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test whether the reloaded model returns the same predictions\n",
    "\n",
    "fill_mask_reloaded = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model_trained_reloaded,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "fill_mask_untrained2(\"Ik ga morgen naar <mask>.\") == fill_mask(\"Ik ga morgen naar <mask>.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have succesfully loaded the trained model. We now want to train it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reload the tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"/home/scur2141/train_tokenizer1/tokenizer\", max_len=512)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Users/jan/Documents/Master/Thesis/Code/Models/model2\", \n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_trained_reloaded, \n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5464256675be84b2b1d9b1da2ca9d2da761aa714131bc1885d646bdbb8cb13ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
